{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b085a502",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "**Logistic Regression** is a probabilistic model that is used for **binary classification** task,the goal of the model is to predict whether an observation belongs to an one of two classes.\n",
    "<br>\n",
    "\n",
    "Unlike **Linear Regression** which predict continous values **Logistic Regression** outputs a probability score from **0 to 1** using a *sigmoid* function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385f718e",
   "metadata": {},
   "source": [
    "## 1.Mathematical Fourmulation\n",
    "\n",
    "We model Logistic regression as :\n",
    "$$\n",
    "z = wx + b\n",
    "$$\n",
    "and then passed to $sigmoid$\n",
    "$$\n",
    "\\hat{y} = \\sigma =\\frac{1}{1+ e^{-z}}\n",
    "$$\n",
    "\n",
    "The reason we use **sigmoid** is that it squashes the output to :\n",
    "$$\n",
    "0<\\hat{y}<1\n",
    "$$\n",
    "<center>\n",
    "<img src = \"https://imgs.search.brave.com/AX86zffYbdnHyGdRjGjxnnc9M14zh3trRhpWLdZKY4M/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9tZWRp/YS5kYXRhY2FtcC5j/b20vY21zL2FkXzRu/eGVpajMzM2E2N3h2/ZmN3b2xzemVrbXBj/amZtdmxjX21rd2M1/Z2tzcDRxbXExOXhm/Y3k0cTVveWkyOW9o/OWJlN2JwMzM0aTZi/Z3h5dmNqcGItb294/LTdlZ3hib2Z6c2I1/YmYzcWp3ZWRvbTZw/bGJjc3d6Y2x6dWts/eGRzYXkwdHZseGNk/MG92NWEucG5n\" alt = \"sigmoid curve\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf143f",
   "metadata": {},
   "source": [
    "## 2. Loss function forumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5ce176",
   "metadata": {},
   "source": [
    "We use **log-loss or binary cross entropy loss** for classification task as it is derived from maximum likelihood estimation.\n",
    "$$\n",
    "L = -\\frac{1}{n} \\sum{[ylog({\\hat{y}})+(1-y)log(1-{\\hat{y}})]}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0611481a",
   "metadata": {},
   "source": [
    "### 2.1 Why not use MSE(Mean Squared Error) loss ?\n",
    "if we used :\n",
    "$$\n",
    "L = (y-\\hat{y})^2\n",
    "$$ \n",
    "Using **Mean Squared Error** as loss function in logistic regression would result in a **non-convex** loss function and unstable optimization. A non-convex function has many minima making it harder in optimization. This happens because we are passing the output of a linear function to a non-linear sigmoid funtion.\n",
    "<br>\n",
    "<center>\n",
    "<img src = \"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*sJaq79557XxTOucSTKJEwA@2x.jpeg\" alt = \"loss surface curve\">\n",
    "</center>\n",
    "\n",
    "We can see a lot of saddle points in MSE, which will cause problems in optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e202364",
   "metadata": {},
   "source": [
    "## 3. Gradients of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd8506",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
